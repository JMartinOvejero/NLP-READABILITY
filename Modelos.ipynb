{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, r2_score\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "\n",
    "def print_metrics(y_true, y_pred):\n",
    "    \n",
    "    print(\"================================================\")\n",
    "    print(\"Spearman's correlation coef: \" + str(spearmanr(y_true, y_pred)[0]))\n",
    "    print(\"================================================\")\n",
    "    \n",
    "    print(\"-----------\")\n",
    "    print(\"R^2 = \" + str(r2_score(y_true, y_pred)))\n",
    "    print(\"R = \" + str(np.sqrt(r2_score(y_true, y_pred))))\n",
    "    print(\"-----------\")\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def grid_search_cv_for_ensembles(model, max_depth_values, n_estimators_values, X, y, scoring_function, k=5, verbose=0):\n",
    "   \n",
    "    \n",
    "    best_score = 0.0\n",
    "    best_n_estimators = 1\n",
    "    best_max_depth = 1\n",
    "    \n",
    "    for max_depth in max_depth_values: \n",
    "        for n_estimators in n_estimators_values:\n",
    "            \n",
    "            kf = KFold(n_splits=k, random_state=None, shuffle=True)\n",
    "\n",
    "            fold = 1\n",
    "            scores = []\n",
    "            for train_index, test_index in kf.split(X):\n",
    "\n",
    "                # get train and test set for the i-th fold\n",
    "                X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "                y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "                # train and predict\n",
    "                model.set_hyperparams(max_depth, n_estimators)\n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred = model.predict(X_test)\n",
    "\n",
    "                scores.append(scoring_function(y_test, y_pred))\n",
    "\n",
    "                fold += 1\n",
    "              \n",
    "            score = np.mean(scores)\n",
    "            \n",
    "            if verbose > 0:\n",
    "                print(\"score=\" + str(score) + \" | max_depth=\" + str(max_depth) + \" n_estimators=\" + str(n_estimators))\n",
    "\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_n_estimators = n_estimators\n",
    "                best_max_depth = max_depth\n",
    "\n",
    "    return best_max_depth, best_n_estimators\n",
    "\n",
    "\n",
    "def find_best_C(model, c_values, X, y, scoring_function, k=5, verbose=0):\n",
    "    \n",
    "    best_score = 0.0\n",
    "    best_c = 1.0\n",
    "    \n",
    "    for c in c_values: \n",
    "            \n",
    "        kf = KFold(n_splits=k, random_state=None, shuffle=True)\n",
    "\n",
    "        fold = 1\n",
    "        scores = []\n",
    "        for train_index, test_index in kf.split(X):\n",
    "\n",
    "            # get train and test set for the i-th fold\n",
    "            X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            # train and predict\n",
    "            model.set_hyperparams('linear', c)\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            scores.append(scoring_function(y_test, y_pred))\n",
    "\n",
    "            fold += 1\n",
    "\n",
    "        score = np.mean(scores)\n",
    "\n",
    "        if verbose > 0:\n",
    "            print(\"score=\" + str(score) + \" | C=\" + str(c))\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_c = c\n",
    "\n",
    "    return best_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def discretize(y_pred):\n",
    "    \n",
    "    \n",
    "    for i in range(len(y_pred)):    \n",
    "        if y_pred[i] < 0.5:\n",
    "            y_pred[i] = 0.0\n",
    "        elif y_pred[i] < 1.5:\n",
    "            y_pred[i] = 1.0\n",
    "        elif y_pred[i] < 2.5:\n",
    "            y_pred[i] = 2.0\n",
    "        elif y_pred[i] < 3.5:\n",
    "            y_pred[i] = 3.0\n",
    "        else:\n",
    "            y_pred[i] = 4.0\n",
    "            \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class RandomForest():\n",
    "  \n",
    "    \n",
    "    def __init__(self, max_depth=20, n_estimators=100, save_model=False, use_saved_model=False, model_path='rf.pickle'):\n",
    "        self.model_path = model_path\n",
    "        self.save_model = save_model\n",
    "        \n",
    "        if use_saved_model:\n",
    "            with open(self.model_path, 'rb') as file:\n",
    "                self.model = pickle.load(file)\n",
    "        else:\n",
    "            self.model = RandomForestRegressor(max_depth=max_depth, n_estimators=n_estimators)    \n",
    "    \n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        self.model.fit(X_train, y_train)\n",
    "        \n",
    "        if self.save_model:\n",
    "            with open(self.model_path, 'wb') as handle:\n",
    "                pickle.dump(self.model, handle)\n",
    "    \n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        return discretize(self.model.predict(X_test))\n",
    "    \n",
    "    \n",
    "    def set_hyperparams(self, max_depth, n_estimators):\n",
    "        \n",
    "        self.model = RandomForestRegressor(max_depth=max_depth, n_estimators=n_estimators)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "class MultilayerPerceptron():\n",
    "  \n",
    "    \n",
    "    def __init__(self, input_dim=None, verbose=0, save_model=False, use_saved_model=False, model_path='mlp.h5'):\n",
    "        self.model_path = model_path\n",
    "        self.save_model = save_model\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.verbose = verbose\n",
    "\n",
    "        \n",
    "        if use_saved_model:\n",
    "            self.model = load_model(model_path)\n",
    "    \n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        self.model = self._make_model()\n",
    "        \n",
    "        y_train_cat = tf.keras.utils.to_categorical(y_train, num_classes=5)\n",
    "        \n",
    "        self.model.fit(\n",
    "            X_train, y_train_cat,\n",
    "            epochs=150,\n",
    "            batch_size=64,\n",
    "            verbose=self.verbose)\n",
    "        \n",
    "        if self.save_model:\n",
    "            self.model.save(self.model_path)\n",
    "    \n",
    "    \n",
    "    def predict(self, X_test): \n",
    "        y_pred_cat = self.model.predict(X_test)\n",
    "        y_pred = np.argmax(y_pred_cat, axis=1)\n",
    "        \n",
    "        return discretize(y_pred)\n",
    "    \n",
    "    \n",
    "    def _make_model(self):\n",
    "        \n",
    "        # architecture\n",
    "        model = Sequential()\n",
    "        model.add(Dense(64, activation='relu', input_dim=self.input_dim))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(5, activation='linear'))\n",
    "        \n",
    "        # opitimizer\n",
    "        adam = tf.keras.optimizers.Adam(lr=0.001)\n",
    "        \n",
    "        model.compile(optimizer=adam,\n",
    "              loss='mse',\n",
    "              metrics=['mse'])\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class XGBoost():\n",
    "    \n",
    "    \n",
    "    def __init__(self, max_depth=30, n_estimators=200, save_model=False, use_saved_model=False, model_path='xgboost.pickle'):\n",
    "        self.model_path = model_path\n",
    "        self.save_model = save_model\n",
    "        \n",
    "        if use_saved_model:\n",
    "            with open(self.model_path, 'rb') as file:\n",
    "                self.model = pickle.load(file)\n",
    "        else:\n",
    "            self.model = xgboost = XGBRegressor(max_depth=max_depth, n_estimators=n_estimators, objective=\"reg:squarederror\")  \n",
    "    \n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        self.model.fit(X_train, y_train)\n",
    "        \n",
    "        if self.save_model:\n",
    "            with open(self.model_path, 'wb') as handle:\n",
    "                pickle.dump(self.model, handle)\n",
    "    \n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        return discretize(self.model.predict(X_test))\n",
    "    \n",
    "    \n",
    "    def set_hyperparams(self, max_depth, n_estimators):\n",
    "       \n",
    "        \n",
    "        self.model = XGBRegressor(max_depth=max_depth, n_estimators=n_estimators, objective=\"reg:squarederror\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SupportVectorMachine():\n",
    " \n",
    "    \n",
    "    def __init__(self, kernel='linear', C=10.0, save_model=False, use_saved_model=False, model_path='svm.pickle'):\n",
    "        self.model_path = model_path\n",
    "        self.save_model = save_model\n",
    "        \n",
    "        if use_saved_model:\n",
    "            with open(self.model_path, 'rb') as file:\n",
    "                self.model = pickle.load(file)\n",
    "        else:\n",
    "            self.model = SVR(kernel=kernel, C=C)\n",
    "    \n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        self.model.fit(X_train, y_train)\n",
    "        \n",
    "        if self.save_model:\n",
    "            with open(self.model_path, 'wb') as handle:\n",
    "                pickle.dump(self.model, handle)\n",
    "    \n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        return discretize(self.model.predict(X_test))\n",
    "    \n",
    "    \n",
    "    def set_hyperparams(self, kernel, c):\n",
    "       \n",
    "        \n",
    "        self.model = SVR(kernel=kernel, C=c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "X_train = pd.read_csv(\"weebit_train_with_features.csv\", index_col=0)\n",
    "X_test = pd.read_csv(\"weebit_test_with_features.csv\", index_col=0)\n",
    "\n",
    "# get Y\n",
    "y_train = X_train[\"Level\"]\n",
    "y_test = X_test[\"Level\"]\n",
    "\n",
    "# remove Y and Text columns \n",
    "X_train.drop(columns=['Text', 'Level'], inplace=True)\n",
    "X_test.drop(columns=['Text', 'Level'], inplace=True)\n",
    "\n",
    "# whole set; used in cross-validation\n",
    "X = pd.concat([X_train, X_test]).reset_index(drop=True)\n",
    "y = pd.concat([y_train, y_test]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score=0.6727937210959603 | max_depth=5 n_estimators=10\n",
      "score=0.7010830544173632 | max_depth=5 n_estimators=50\n",
      "score=0.6871396643175932 | max_depth=5 n_estimators=100\n",
      "score=0.7144643305955798 | max_depth=10 n_estimators=10\n",
      "score=0.7483417235339506 | max_depth=10 n_estimators=50\n",
      "score=0.7551947294969076 | max_depth=10 n_estimators=100\n",
      "score=0.7163050154274743 | max_depth=15 n_estimators=10\n",
      "score=0.7466707942146229 | max_depth=15 n_estimators=50\n",
      "score=0.7561007116148158 | max_depth=15 n_estimators=100\n",
      "score=0.7000136596460722 | max_depth=20 n_estimators=10\n",
      "score=0.7436165639474487 | max_depth=20 n_estimators=50\n",
      "score=0.761314858219543 | max_depth=20 n_estimators=100\n",
      "\n",
      "Mejores hiperpar치metros son: max_depth=20 n_estimators=100\n"
     ]
    }
   ],
   "source": [
    "scoring_function = lambda y_true, y_pred: spearmanr(y_true, y_pred)[0]\n",
    "max_depth_values = [5, 10, 15, 20]\n",
    "n_estimators_values = [10, 50, 100]\n",
    "\n",
    "max_depth, n_estimators = grid_search_cv_for_ensembles(RandomForest(), max_depth_values, n_estimators_values, X_train, y_train, scoring_function, k=3, verbose=1)\n",
    "\n",
    "print()\n",
    "print(\"Mejores hiperpar치metros son: max_depth=\" + str(max_depth) + \" n_estimators=\" + str(n_estimators))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================\n",
      "Spearman's correlation coef: 0.6631028976774571\n",
      "================================================\n",
      "-----------\n",
      "R^2 = 0.43731577492319496\n",
      "R = 0.6612985520347031\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForest(max_depth=max_depth, n_estimators=n_estimators, save_model=True)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "print_metrics(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score=0.7217380217276164 | max_depth=5 n_estimators=10\n",
      "score=0.7511812564494326 | max_depth=5 n_estimators=50\n",
      "score=0.7428138700219561 | max_depth=5 n_estimators=100\n",
      "score=0.7526284555324304 | max_depth=5 n_estimators=200\n",
      "score=0.6975442755384388 | max_depth=10 n_estimators=10\n",
      "score=0.6952075050462417 | max_depth=10 n_estimators=50\n",
      "score=0.7123745459327208 | max_depth=10 n_estimators=100\n",
      "score=0.7030752030529602 | max_depth=10 n_estimators=200\n",
      "score=0.6944836602959455 | max_depth=15 n_estimators=10\n",
      "score=0.6804872131434797 | max_depth=15 n_estimators=50\n",
      "score=0.69516659500275 | max_depth=15 n_estimators=100\n",
      "score=0.7111820103464739 | max_depth=15 n_estimators=200\n",
      "score=0.6935425335460758 | max_depth=20 n_estimators=10\n",
      "score=0.6936676766614323 | max_depth=20 n_estimators=50\n",
      "score=0.6978170748606759 | max_depth=20 n_estimators=100\n",
      "score=0.6856781404788693 | max_depth=20 n_estimators=200\n",
      "score=0.6800584971530986 | max_depth=30 n_estimators=10\n",
      "score=0.6927421973892133 | max_depth=30 n_estimators=50\n",
      "score=0.6985787267224923 | max_depth=30 n_estimators=100\n",
      "score=0.700513182012899 | max_depth=30 n_estimators=200\n",
      "\n",
      "Mejores hiperpar치metros son: max_depth=5 n_estimators=200\n"
     ]
    }
   ],
   "source": [
    "\n",
    "max_depth_values = [5, 10, 15, 20, 30]\n",
    "n_estimators_values = [10, 50, 100, 200]\n",
    "\n",
    "\n",
    "max_depth, n_estimators = grid_search_cv_for_ensembles(XGBoost(), max_depth_values, n_estimators_values, X_train, y_train, scoring_function, k=3, verbose=1)\n",
    "\n",
    "print()\n",
    "print(\"Mejores hiperpar치metros son: max_depth=\" + str(max_depth) + \" n_estimators=\" + str(n_estimators))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================\n",
      "Spearman's correlation coef: 0.6874196870544353\n",
      "================================================\n",
      "-----------\n",
      "R^2 = 0.455032532639719\n",
      "R = 0.6745609925275245\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "xgboost = XGBoost(save_model=True)\n",
    "\n",
    "xgboost.fit(X_train, y_train)\n",
    "y_pred = xgboost.predict(X_test)\n",
    "print_metrics(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score=0.7136750274997627 | C=1.0\n",
      "score=0.7269684259553048 | C=2.0\n",
      "score=0.7301882882433711 | C=5.0\n",
      "score=0.7293531806037441 | C=10.0\n",
      "score=0.733927492529622 | C=20.0\n",
      "\n",
      "El mejor C es 20.0\n"
     ]
    }
   ],
   "source": [
    "c_values = [1.0, 2.0, 5.0, 10.0, 20.0]\n",
    "\n",
    "best_c = find_best_C(SupportVectorMachine(), c_values, X_train, y_train, scoring_function, k=3, verbose=1)\n",
    "\n",
    "print()\n",
    "print(\"El mejor C es \" + str(best_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================\n",
      "Spearman's correlation coef: 0.7112324607545297\n",
      "================================================\n",
      "-----------\n",
      "R^2 = 0.48479668560347955\n",
      "R = 0.6962734273282871\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "svm = SupportVectorMachine(C=best_c, save_model=True)\n",
    "\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "print_metrics(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\susi0\\anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================\n",
      "Spearman's correlation coef: 0.713240039046694\n",
      "================================================\n",
      "-----------\n",
      "R^2 = 0.43448109368855103\n",
      "R = 0.6591517986689797\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mlp = MultilayerPerceptron(input_dim=X_train.shape[1], save_model=True, verbose=0)\n",
    "\n",
    "mlp.fit(X_train, y_train)\n",
    "y_pred = mlp.predict(X_test)\n",
    "print_metrics(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
